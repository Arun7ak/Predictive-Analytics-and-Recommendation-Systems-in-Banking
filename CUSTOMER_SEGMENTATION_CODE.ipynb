{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES FOR HANDLING AND GENERATING FAKE DATAFRAME\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERTING FAKE DATA\n",
    "fake = Faker()\n",
    "\n",
    "num_customers = 2500\n",
    "max_transactions_per_customer = 20 \n",
    "\n",
    "customers = [f\"CUST{str(i).zfill(6)}\" for i in range(1, num_customers + 1)]\n",
    "\n",
    "transaction_data = []\n",
    "for customer_id in customers:\n",
    "    num_transactions = random.randint(1, max_transactions_per_customer)\n",
    "    for _ in range(num_transactions):\n",
    "        transaction_data.append({\n",
    "            \"customer_id\": customer_id,\n",
    "            \"transaction_id\": f\"TRANS{str(len(transaction_data) + 1).zfill(6)}\",\n",
    "            \"transaction_amount\": np.random.uniform(10, 1000),\n",
    "            \"transaction_type\": random.choice([\"deposit\", \"withdrawal\"]),\n",
    "            \"transaction_date\": fake.date_between(start_date='-1y', end_date='today'),\n",
    "        })\n",
    "\n",
    "transaction_data = pd.DataFrame(transaction_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPLAY THE DATAFRAME\n",
    "transaction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE TOTAL TRANSACTION AND COUNT THE TOTAL CUSTOMER \n",
    "print(f\"Total transactions: {len(transaction_data)}\")\n",
    "print(f\"Unique customers: {transaction_data['customer_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE NULL VALES \n",
    "transaction_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING DUPLICATE VALUE\n",
    "transaction_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK THE INFO OF DATAFRAME\n",
    "transaction_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DESCRIBE THE DATAFRAME \n",
    "transaction_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE ALL COLUMNS COUNTS\n",
    "transaction_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK THE DATATYPE OF THE DATAFRAME\n",
    "transaction_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK THE VALUE COUNT IN TRANSACTIO TYPE COLUMN\n",
    "transaction_data['transaction_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES FOR EDA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE OUTLIER\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.boxplot(data=transaction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERTING THE COLUMN TO CORRECT DATATYPE\n",
    "transaction_data['transaction_date'] = pd.to_datetime(transaction_data['transaction_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERTING THE RUPEES TO DOLLAR\n",
    "transaction_data[\"transaction_amount\"] = transaction_data[\"transaction_amount\"] / 86.66\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROUNDING THE COLUMNS VALUES\n",
    "transaction_data['transaction_amount'] = transaction_data['transaction_amount'].round(1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPLAY THE DATAFRAME\n",
    "transaction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING NEW COLUMNS\n",
    "elements = ['withdrawal','deposit']\n",
    "\n",
    "for element in elements:\n",
    "    Tranformed_Columns = transaction_data['transaction_type'] == element\n",
    "    transaction_data[f'{element}_Value'] = transaction_data['transaction_amount'].where(Tranformed_Columns,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILLING THE NULL WITH 0 FOR SUM IT\n",
    "transaction_data.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GROUPING THE COLUMNS FOR TRAINING THE MODEL MORE EFFICIENTLY\n",
    "customer_data = transaction_data.groupby(\"customer_id\").agg(\n",
    "    total_transactions=(\"transaction_id\", \"count\"),\n",
    "    total_amount=(\"transaction_amount\", \"sum\"),\n",
    "    num_deposits=(\"transaction_type\", lambda x: (x == \"deposit\").sum()),\n",
    "    num_withdrawals=(\"transaction_type\", lambda x: (x == \"withdrawal\").sum()),\n",
    "    withdrawals_amount=(\"withdrawal_Value\",'sum'),\n",
    "    deposits_amount=(\"deposit_Value\",'sum')\n",
    ").reset_index()\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGAIN CHECK THE DATATYPE AFTER GROUPING\n",
    "customer_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE NULL VALUE AFTER GROUPING\n",
    "customer_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TOP 10 CUSTOMERS BASED ON TOTAL AMOUNT IN DESCENDING ORDER\n",
    "top_10 = customer_data[[\"customer_id\",\"total_transactions\",\"total_amount\"]].sort_values(by='total_amount', ascending=False).head(10)\n",
    "plt.figure(figsize=(13, 5))\n",
    "sns.barplot(y=\"customer_id\", x=\"total_amount\", data=top_10, orient='h')\n",
    "plt.xlabel(\"Total Amount\")\n",
    "plt.ylabel(\"Customer ID\")\n",
    "plt.title(\"Top 10 Customers by Total Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT TOP 10 CUSTOMERS BASED ON WITHDRAWALS AMOUNT \n",
    "top_10 = customer_data[[\"customer_id\",\"withdrawals_amount\",\"deposits_amount\"]].sort_values(by='withdrawals_amount', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "sns.barplot(x=\"customer_id\", y=\"withdrawals_amount\", data=top_10, palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 Customers by Withdrawals Amount\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Customer ID\", fontsize=14)\n",
    "plt.ylabel(\"Withdrawals Amount\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A DISTRIBUTION PLOT FOR TOTAL AMOUNT WITH KDE\n",
    "sns.displot(data=customer_data, x=\"total_amount\", kde=True)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TOP 10 CUSTOMERS BASED ON DEPOSITS AMOUNT \n",
    "top_10 = customer_data[[\"customer_id\",\"withdrawals_amount\",\"deposits_amount\"]].sort_values(by='deposits_amount', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "sns.barplot(x=\"customer_id\", y=\"deposits_amount\", data=top_10, palette=\"Greens\")\n",
    "plt.title(\"Top 10 Customers by Deposits Amount\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Customer ID\", fontsize=14)\n",
    "plt.ylabel(\"Deposits Amount\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING OUTLIER FOR ALL THE COLUMNS\n",
    "plt.figure(figsize=(16, 5))\n",
    "sns.boxplot(data = customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE CORRELATION FOR NUMERIC COLUMNS USING HEATMAP\n",
    "correlation_matrix = customer_data[[\"total_transactions\", \"total_amount\", \n",
    "                                    \"num_deposits\", \"num_withdrawals\", \"withdrawals_amount\", \"deposits_amount\"]].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTED ALL THE LIBRARIES FOR TRAINING,EVALUATE THE MODLE AND EVALUATE THE MODEL\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENCODE THE TRAINING DATA COLUMNS\n",
    "features = [\"total_transactions\", \"total_amount\", \"num_deposits\", \"num_withdrawals\",\"withdrawals_amount\",\"deposits_amount\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(customer_data[features])\n",
    "\n",
    "scaled_customer_data = pd.DataFrame(scaled_features, columns=features)\n",
    "scaled_customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE MODEL USING KMEANS ALGORITHM\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "customer_data[\"cluster\"] = kmeans.fit_predict(scaled_customer_data)\n",
    "\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERTUNING THE CENTROID BASED K MEANS CLUSTERING MODEL \n",
    "kmeans = KMeans(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_clusters': np.arange(2, 11), \n",
    "    'init': ['k-means++', 'random'],  \n",
    "    'max_iter': [300, 500, 1000],  \n",
    "    'n_init': [10, 15],  \n",
    "    'tol': [1e-4, 1e-3],  \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(kmeans, param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(scaled_customer_data)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "best_kmeans = KMeans(**grid_search.best_params_, random_state=42)\n",
    "customer_data[\"cluster\"] = best_kmeans.fit_predict(scaled_customer_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY THE DATAFRAME AFTER CLUSTERING\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK THE SCORE FOR KMEANS MODEL\n",
    "silhouette = silhouette_score(scaled_features, customer_data[\"cluster\"])\n",
    "db_index = davies_bouldin_score(scaled_features, customer_data[\"cluster\"])\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPLAY THE PCA PLOT KMEANS MODEL\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(scaled_features)\n",
    "\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=customer_data[\"cluster\"], cmap='viridis', s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200)\n",
    "plt.title('Cluster Visualization (PCA)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING JOBLIB TO SAVE THE MODEL\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVED THE GOOD SCORE MODEL\n",
    "joblib.dump(kmeans, r\"d:\\BANK PROJECT\\kmeans.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE THE SCALED DATA \n",
    "joblib.dump(scaled_customer_data,r\"d:\\BANK PROJECT\\scaled d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE DENSITY BASED DBSCAN MODEL\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  \n",
    "customer_data[\"cluster\"] = dbscan.fit_predict(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK THE SCORE FOR DBSCAN MODEL\n",
    "if len(set(customer_data[\"cluster\"])) > 1:\n",
    "    silhouette = silhouette_score(scaled_features, customer_data[\"cluster\"])\n",
    "    db_index = davies_bouldin_score(scaled_features, customer_data[\"cluster\"])\n",
    "    print(f\"Silhouette Score: {silhouette}\")\n",
    "    print(f\"Davies-Bouldin Index: {db_index}\")\n",
    "else:\n",
    "    print(\"DBSCAN found less than two clusters; Silhouette Score is not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPLAY THE PCA PLOT FOR DBSCAN MODEL\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(scaled_features)\n",
    "\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=customer_data[\"cluster\"], cmap='viridis', s=50)\n",
    "plt.title('Cluster Visualization (DBSCAN, PCA)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINED THE AGGLOMERATIVE CLUSTERING\n",
    "hc = AgglomerativeClustering(n_clusters=2)\n",
    "scaled_customer_data['cluster'] = hc.fit_predict(scaled_customer_data)\n",
    "\n",
    "sch.dendrogram(sch.linkage(scaled_customer_data[features], method='ward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK THE SCORE FOR AGGLOMERATIVE CLUSTERING\n",
    "silhouette = silhouette_score(scaled_features, scaled_customer_data[\"cluster\"])\n",
    "db_index = davies_bouldin_score(scaled_features, scaled_customer_data[\"cluster\"])\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPLAY THE PCA PLOT FOR AGGLOMERATIVE CLUSTERING\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(scaled_features)\n",
    "\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=scaled_customer_data[\"cluster\"], cmap='viridis', s=50)\n",
    "plt.title('Cluster Visualization (PCA)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
